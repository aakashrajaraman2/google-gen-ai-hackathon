{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_key= os.getenv(\"CLAUDE_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths =[\n",
    "    \"docs/auto.pdf\",\n",
    "    \"docs/health.pdf\",\n",
    "    \"docs/home insurance.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [PyPDFLoader(url).load() for url in paths]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aakash Rajaraman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "\n",
    "def lanceDBConnection(embed):\n",
    "    db = lancedb.connect(\"/tmp/lancedb\")\n",
    "    table = db.create_table(\n",
    "        \"crag_demo\",\n",
    "        data=[{\"vector\": embed.embed_query(\"Hello World\"), \"text\": \"Hello World\"}],\n",
    "        mode=\"overwrite\",\n",
    "    )\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "table = lanceDBConnection(model)\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=model,\n",
    "    connection=table,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aakash Rajaraman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3550: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, TypedDict\n",
    "\n",
    "from typing import TypedDict\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import  Field\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(state):#Node 1. will act as a tool\n",
    "    \"\"\"\n",
    "    Helper function for retrieving documents. \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"*\" * 5, \" RETRIEVE \", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question}}#return the same state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "#llm = ChatAnthropic(model_name=\"claude-3-5-sonnet-20240620\",api_key=claude_key, streaming=True, model_kwargs=dict(system='You must complete several tasks '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "import os \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"cred.json\"\n",
    "#os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "vertexai.init(project=\"vision-forge-414908\", location=\"us-central1\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-001\",\n",
    "                             system_instruction=\"You will be given various tasks in the following prompts. Remember that you must be as elaborate and professional as possible. Understand the task carefully, then respond\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grade_documents(state):#node 2\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a policy document to a user question. \\n\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document relates to the user question, grade it as relevant. \\n\n",
    "        Ensure that the user's question's specific policy question matches the policy type (auto, pet, life, etc) in the documents.\n",
    "        Only give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "        Only answer with the words 'yes' or 'no'. Do not provide any other text. \\n\\n\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm \n",
    "    print(\"here2\")\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    search = \"No\"  # Default do not opt for web search to supplement retrieval\n",
    "    for d in documents:\n",
    "        score = chain.invoke({\"question\": question, \"context\": d.page_content})\n",
    "        score = [score.content.replace(\"\\n\", \"\")]\n",
    "        print(\"THIS IS THE SCORE:\", score)\n",
    "        grade2 = score[0]\n",
    "        if \"yes\" in grade2 or \"Yes\" in grade2:\n",
    "            print(\"*\" * 5, \" RATED DOCUMENT: RELEVANT\", \"*\" * 5)\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"*\" * 5, \" RATED DOCUMENT: NOT RELEVANT\", \"*\" * 5)\n",
    "            continue\n",
    "        \n",
    "        #if not even half the docs are relevant\n",
    "    if len(filtered_docs) < int(len(documents)/2):\n",
    "        search = \"Yes\"\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "            \"run_web_search\": search,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):#node 3. also, end.\n",
    "    \"\"\"\n",
    "    Helper function for generating answers\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"*\" * 5, \" GENERATE \", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "    template='''\n",
    "    You are an assistant for insurance related question-answering tasks. \n",
    "    Use the following pieces of insurance policies context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Give as much information as possible. Use a professional tone, and elaborate as much as you can.\n",
    "\n",
    "    Question: {question} \n",
    "\n",
    "    Context: {context} \n",
    "\n",
    "    Answer:\n",
    "    ''',\n",
    "    input_variables=[\"question\", \"context\"],)\n",
    "\n",
    "    # LLM\n",
    "\n",
    "\n",
    "    # RAG Chain\n",
    "    rag_chain = prompt | llm \n",
    "\n",
    "    # Run generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(state):#node 4\n",
    "    \"\"\"\n",
    "    Helper function for transforming the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"*\" * 5, \"TRANSFORM QUERY\", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n\n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n\n",
    "        You have to modify the search query to only look for Indian related results. \n",
    "        Only return the question, no further explanation or text.\\n\n",
    "        Here is the initial question:\n",
    "        \\n --------- \\n\n",
    "        {question}\n",
    "        \\n --------- \\n\n",
    "        Formulate an improved question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    model = llm\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "    print(better_question)\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": better_question}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):#node 5\n",
    "    \"\"\"\n",
    "    Helper function to determine whether to generate an answer or re-generate a question for web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"*\" * 5, \" DECIDE TO GENERATE \", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    search = state_dict[\"run_web_search\"]\n",
    "\n",
    "    if \"yes\" in search or \"Yes\" in search:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"*\" * 5, \" DECISION: TRANSFORM QUERY and RUN WEB SEARCH \", \"*\" * 5)\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"*\" * 5, \" DECISION: GENERATE \", \"*\" * 5)\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state):#node 6\n",
    "    \"\"\"\n",
    "    Helper function to do Web search based on the re-phrased question using Tavily API.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"*\" * 5, \" WEB SEARCH \", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    tool = tavily_client.search(    query=question,\n",
    "                                    search_depth=\"advanced\",\n",
    "                                    include_answer=True,\n",
    "                                    include_domains=[\"https://www.acko.com/car-insurance/irdai-rules/\", \"https://www.lexisnexis.in/blogs/insurance-law-in-india/.\"])\n",
    "\n",
    "    docs = tool[\"answer\"]\n",
    "    #web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    #print(web_results)\n",
    "    print(tool)\n",
    "    web_results = Document(page_content=docs)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)#inherit empty state\n",
    "\n",
    "# Define the nodes\n",
    "#nodes work by (node_name, function_of_node)\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve docs\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade retrieved docs\n",
    "workflow.add_node(\"generate\", generate)  # generate answers\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query for web search\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(#conditional edges are based on a condition, which is a function that returns a boolean value\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****  RETRIEVE  *****\n",
      "here2\n",
      "THIS IS THE SCORE: ['no ']\n",
      "*****  RATED DOCUMENT: NOT RELEVANT *****\n",
      "THIS IS THE SCORE: ['no ']\n",
      "*****  RATED DOCUMENT: NOT RELEVANT *****\n",
      "THIS IS THE SCORE: ['no']\n",
      "*****  RATED DOCUMENT: NOT RELEVANT *****\n",
      "THIS IS THE SCORE: ['no']\n",
      "*****  RATED DOCUMENT: NOT RELEVANT *****\n",
      "*****  DECIDE TO GENERATE  *****\n",
      "*****  DECISION: TRANSFORM QUERY and RUN WEB SEARCH  *****\n",
      "***** TRANSFORM QUERY *****\n",
      "Are there any restrictions on dog breeds for insurance in India? \n",
      "\n",
      "*****  WEB SEARCH  *****\n",
      "{'query': 'Are there any restrictions on dog breeds for insurance in India? \\n', 'follow_up_questions': None, 'answer': 'As of the most recent data available, there are no specific restrictions on dog breeds for insurance in India. Insurers may have their own policies and guidelines regarding coverage for certain breeds, so it is advisable to check with individual insurance companies for any breed-specific restrictions.', 'images': [], 'results': [], 'response_time': 15.93}\n",
      "*****  GENERATE  *****\n"
     ]
    }
   ],
   "source": [
    "op = app.invoke({\"keys\": {\"question\": \"is there a restriction of the type of dogs i can apply for insurance?\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dict(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op=x[\"keys\"][\"generation\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The maximum length of stay allowed in Indian hospitals can vary depending on the specific hospital and the medical condition of the patient. There is no standard nationwide limit set by the government. It is recommended to check with the specific hospital or healthcare provider for their policies on length of stay. \\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
