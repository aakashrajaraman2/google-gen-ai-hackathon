{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ""
     ]
    }
   ],
   "source": [
    "claude_key= os.getenv(\"CLAUDE_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths =[\n",
    "    \"90ade7e39d5e481f9aeb772a19a30234.pdf\",\n",
    "    \"English Health Handbook.pdf\",\n",
    "    \"English Motor Handbook.pdf\",\n",
    "    \"insurance_motor_car_motor_policy_booklet_241017_NMDMG10249_v3.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [PyPDFLoader(url).load() for url in paths]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "\n",
    "def lanceDBConnection(embed):\n",
    "    db = lancedb.connect(\"/tmp/lancedb\")\n",
    "    table = db.create_table(\n",
    "        \"crag_demo\",\n",
    "        data=[{\"vector\": embed.embed_query(\"Hello World\"), \"text\": \"Hello World\"}],\n",
    "        mode=\"overwrite\",\n",
    "    )\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "table = lanceDBConnection(model)\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=model,\n",
    "    connection=table,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "import langsmith\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(state):#Node 1. will act as a tool\n",
    "    \"\"\"\n",
    "    Helper function for retrieving documents. \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"*\" * 5, \" RETRIEVE \", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question}}#return the same state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model_name=\"claude-3-5-sonnet-20240620\",api_key=claude_key, streaming=True, model_kwargs=dict(system='You must complete several tasks '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grade_documents(state):#node 2\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a policy document to a user question. \\n\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document relates to the user question, grade it as relevant. \\n\n",
    "        Ensure that the user's question's specific policy question matches the policy type (auto, pet, life, etc) in the documents.\n",
    "        Only give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "        Only answer with the words 'yes' or 'no'. Do not provide any other text. \\n\\n\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm \n",
    "    print(\"here2\")\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    search = \"No\"  # Default do not opt for web search to supplement retrieval\n",
    "    for d in documents:\n",
    "        score = chain.invoke({\"question\": question, \"context\": d.page_content})\n",
    "        score = [score.content]\n",
    "        print(\"THIS IS THE SCORE:\", score)\n",
    "        grade2 = score[0]\n",
    "        if grade2 == \"yes\":\n",
    "            print(\"*\" * 5, \" RATED DOCUMENT: RELEVANT\", \"*\" * 5)\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"*\" * 5, \" RATED DOCUMENT: NOT RELEVANT\", \"*\" * 5)\n",
    "            continue\n",
    "        \n",
    "        #if not even half the docs are relevant\n",
    "        if len(filtered_docs) < int(len(documents)/2):\n",
    "            search = \"Yes\"\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "            \"run_web_search\": search,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):#node 3. also, end.\n",
    "    \"\"\"\n",
    "    Helper function for generating answers\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"*\" * 5, \" GENERATE \", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "    template='''\n",
    "    You are an assistant for insurance related question-answering tasks. \n",
    "    Use the following pieces of insurance policies context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Give as much information as possible. Use a professional tone, and elaborate as much as you can.\n",
    "\n",
    "    Question: {question} \n",
    "\n",
    "    Context: {context} \n",
    "\n",
    "    Answer:\n",
    "    ''',\n",
    "    input_variables=[\"question\", \"context\"],)\n",
    "\n",
    "    # LLM\n",
    "\n",
    "\n",
    "    # RAG Chain\n",
    "    rag_chain = prompt | llm \n",
    "\n",
    "    # Run generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(state):#node 4\n",
    "    \"\"\"\n",
    "    Helper function for transforming the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"*\" * 5, \"TRANSFORM QUERY\", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n\n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n\n",
    "        Only return the question, no further explanation or text.\\n\n",
    "        Here is the initial question:\n",
    "        \\n --------- \\n\n",
    "        {question}\n",
    "        \\n --------- \\n\n",
    "        Formulate an improved question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    model = llm\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "    print(better_question)\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": better_question}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):#node 5\n",
    "    \"\"\"\n",
    "    Helper function to determine whether to generate an answer or re-generate a question for web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"*\" * 5, \" DECIDE TO GENERATE \", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    search = state_dict[\"run_web_search\"]\n",
    "\n",
    "    if search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"*\" * 5, \" DECISION: TRANSFORM QUERY and RUN WEB SEARCH \", \"*\" * 5)\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"*\" * 5, \" DECISION: GENERATE \", \"*\" * 5)\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient(api_key=\"tvly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state):#node 6\n",
    "    \"\"\"\n",
    "    Helper function to do Web search based on the re-phrased question using Tavily API.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"*\" * 5, \" WEB SEARCH \", \"*\" * 5)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    tool = tavily_client.search(    query=question,\n",
    "                                    search_depth=\"advanced\",\n",
    "                                    include_answer=True,\n",
    "                                    include_domains=[\"https://www.acko.com/car-insurance/irdai-rules/\", \"https://www.lexisnexis.in/blogs/insurance-law-in-india/.\"])\n",
    "\n",
    "    docs = tool[\"answer\"]\n",
    "    #web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    #print(web_results)\n",
    "    web_results = Document(page_content=docs)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)#inherit empty state\n",
    "\n",
    "# Define the nodes\n",
    "#nodes work by (node_name, function_of_node)\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve docs\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade retrieved docs\n",
    "workflow.add_node(\"generate\", generate)  # generate answers\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query for web search\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(#conditional edges are based on a condition, which is a function that returns a boolean value\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****  RETRIEVE  *****\n",
      "here2\n",
      "THIS IS THE SCORE: ['yes']\n",
      "*****  RATED DOCUMENT: RELEVANT *****\n",
      "THIS IS THE SCORE: ['no']\n",
      "*****  RATED DOCUMENT: NOT RELEVANT *****\n",
      "THIS IS THE SCORE: ['yes']\n",
      "*****  RATED DOCUMENT: RELEVANT *****\n",
      "THIS IS THE SCORE: ['no']\n",
      "*****  RATED DOCUMENT: NOT RELEVANT *****\n",
      "*****  DECIDE TO GENERATE  *****\n",
      "*****  DECISION: TRANSFORM QUERY and RUN WEB SEARCH  *****\n",
      "***** TRANSFORM QUERY *****\n",
      "What is the maximum allowed length of stay for patients in a hospital?\n",
      "*****  WEB SEARCH  *****\n",
      "*****  GENERATE  *****\n"
     ]
    }
   ],
   "source": [
    "op = app.invoke({\"keys\": {\"question\": \"How long can I stay in the hospital, maximum?\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dict(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op=x[\"keys\"][\"generation\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, there is no specific maximum allowed length of stay mentioned for patients in a hospital. The policy documents do not define a strict limit on the duration of hospitalization.\\n\\nHowever, the context does provide some relevant information about hospitalization:\\n\\n1. The policy defines \"Hospitalisation\" as admission in a Hospital for a minimum period of 24 consecutive hours of In-Patient Care, except for specified procedures/treatments where admission could be for less than 24 consecutive hours.\\n\\n2. The policy covers \"Inpatient Care,\" which is defined as treatment for which the insured person has to stay in a hospital for more than 24 hours for a covered event.\\n\\n3. The policy emphasizes \"Medically Necessary\" treatment, which is defined as treatment that:\\n   a) Is required for the medical management of the illness or injury\\n   b) Must not exceed the level of care necessary to provide safe, adequate, and appropriate medical care\\n   c) Must have been prescribed by a medical practitioner\\n   d) Must conform to professional standards widely accepted in international medical practice or by the medical community in India\\n\\nGiven these points, it appears that the length of stay is determined based on medical necessity rather than a fixed maximum duration. The policy focuses on ensuring that the hospitalization is appropriate for the patient\\'s condition and does not exceed what is medically required.\\n\\nIt\\'s important to note that specific length of stay limitations may vary depending on the individual case, the type of treatment, and the policies of the particular hospital or insurance provider. For the most accurate information regarding maximum allowed length of stay, it would be best to consult directly with the insurance provider or the specific hospital in question.'"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
